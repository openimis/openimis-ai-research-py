{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simona/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "\n",
    "# Constants definition:\n",
    "CS_Rejected = 1\n",
    "CS_Entered  = 2\n",
    "CS_Checked  = 4\n",
    "CS_Valuated = 16\n",
    "\n",
    "RS_Idle = 1\n",
    "RS_NotSelected = 2\n",
    "RS_SelectedForReview = 4\n",
    "RS_Reviewed = 8\n",
    "RS_Bypassed = 16\n",
    "\n",
    "#RR:RejectionReason\n",
    "RR_Accepted = 0 \n",
    "RR_RbyMO    = -1\n",
    "\n",
    "#CIS:ClaimItemStatus\n",
    "CIS_Accepted = 1\n",
    "CIS_Rejected = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Note**: the data preparation step covers several tasks related to\n",
    "- Data concatenation\n",
    "- **Sanity check**: verify if all the data is coherent and exclude incoherencies;\n",
    "- Filling missing values in the dataset: filling strategies specific to each column\n",
    "- Converting categorical data to numeric: date and text variables\n",
    "- Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check\n",
    "The present file is dealing with the sanity check of the concatenated dataset.\n",
    "In this step, we define the excluding conditions, related to several or all of the data fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the ftr file with all concatenated data\n",
    "df_claim_si_concat = pd.read_feather('openIMIS csv/AllData.ftr')\n",
    "\n",
    "# save the memory and shape dimensions:\n",
    "memStats_claims = (df_claim_si_concat.memory_usage()/1024/1024).sum()\n",
    "shape_claims = df_claim_si_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of excluding conditions and add another column with the associated conditions\n",
    "\n",
    "# The items that are submitted, but not yet in checked by the Rule Engile are excluded\n",
    "# Indeed, only items accepted by the Rule Engine can constitue an input for the AI\n",
    "exclusion_cnd1 = df_claim_si_concat['ClaimStatus']==CS_Entered\n",
    "\n",
    "# Items rejected by the Rule Engine are excluded\n",
    "exclusion_cnd2 = df_claim_si_concat['RejectionReason']>RR_Accepted\n",
    "\n",
    "# Missing values in the ClaimAdminId, PolicyID, ProdID, VisiType fields\n",
    "exclusion_cnd3 = (df_claim_si_concat['ClaimAdminId'].isnull())|\\\n",
    "(df_claim_si_concat['PolicyID'].isnull())|\\\n",
    "(df_claim_si_concat['ProdID'].isnull())|\\\n",
    "(df_claim_si_concat['VisitType'].isnull())\n",
    "\n",
    "# Incoherence in the status fields are excluded\n",
    "# - items with ClaimItemStatus == Rejected and RejectionReason == Accepted\n",
    "# - items with ClaimStatus == Rejected and RejectionReason == Accepted\n",
    "# - items with ClaimItemStatus == Accepted and RejectionReason == Rejected by Medical Officer\n",
    "exclusion_cnd4 = ((df_claim_si_concat['ClaimItemStatus']==CIS_Rejected)&\\\n",
    "                  (df_claim_si_concat['RejectionReason']==RR_Accepted))|\\\n",
    "((df_claim_si_concat['ClaimStatus']==CS_Rejected)&(df_claim_si_concat['RejectionReason']==RR_Accepted))|\\\n",
    "((df_claim_si_concat['ClaimItemStatus']==CIS_Accepted)&(df_claim_si_concat['RejectionReason']==RR_RbyMO))\n",
    "\n",
    "# Incoherence in the date related fields\n",
    "# - items with DateFrom before 15-05-2016\n",
    "# - items with DOB before the DateClaimed\n",
    "# - items with DateClaimed before 15-05-2016\n",
    "# - items with DateClaimed before DateFrom\n",
    "exclusion_cnd5 = (df_claim_si_concat['DateFrom']<datetime.datetime(2016, 5, 15))|\\\n",
    "(df_claim_si_concat['DOB']>df_claim_si_concat['DateClaimed'])|\\\n",
    "(df_claim_si_concat['DateClaimed']<datetime.datetime(2016, 5, 15))|\\\n",
    "(df_claim_si_concat['DateClaimed']<df_claim_si_concat['DateFrom'])\n",
    "\n",
    "# Incoherence between status and valuated price\n",
    "exclusion_cnd6 =(df_claim_si_concat['RejectionReason']==RR_RbyMO)&(df_claim_si_concat['PriceValuated']>0)\n",
    "\n",
    "# Check if ClaimAdminID has the same HFID as the ClaimHFID:\n",
    "exclusion_cnd7 = (df_claim_si_concat['HFID']!=df_claim_si_concat['HFId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate these conditions to items in the dataset:\n",
    "\n",
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (exclusion_cnd1),\n",
    "    (exclusion_cnd2),\n",
    "    (exclusion_cnd3),\n",
    "    (exclusion_cnd4),\n",
    "    (exclusion_cnd5),\n",
    "    (exclusion_cnd6),\n",
    "    (exclusion_cnd7),\n",
    "    ~(exclusion_cnd1&exclusion_cnd2&exclusion_cnd3&\\\n",
    "      exclusion_cnd4&exclusion_cnd5&exclusion_cnd6&exclusion_cnd7)\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['Condition1', 'Condition2','Condition3','Condition4','Condition5',\\\n",
    "          'Condition6','Condition7','Clean data']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "df_claim_si_concat['SanityCheck']=np.select(conditions, values)\n",
    "\n",
    "# if necessary, save the curent data:\n",
    "#df_claim_si_concat.to_pickle('openIMIS csv/AllData_ExclusionConds.pkl') \n",
    "#df_claim_si_concat.to_csv('openIMIS csv/AllData_ExclusionConds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim_si_concat['SanityCheck'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation: illustrate the number of items per excluding condition\n",
    "\n",
    "figs = plt.figure(figsize=(20,5))\n",
    "\n",
    "df_selection = df_claim_si_concat.loc[df_claim_si_concat['SanityCheck']!='Clean data']\n",
    "items_count  = df_selection['SanityCheck'].value_counts().sort_index()\n",
    "sns.barplot(items_count.index, items_count.values, alpha=0.8)\n",
    "plt.title(\"Items per exclusion conditions\")\n",
    "plt.xticks(range(6),['Condition1', 'Condition2','Condition3','Condition4','Condition5','Condition6'])\n",
    "plt.ylabel(\"# of items\")\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clean dataset to another dataframe\n",
    "df_cleandata = df_claim_si_concat.loc[df_claim_si_concat['SanityCheck']=='Clean data'].copy()\n",
    "\n",
    "memStats_cleandata = (df_cleandata.memory_usage()/1024/1024).sum()\n",
    "shape_cleandata = df_cleandata.shape\n",
    "\n",
    "# if necessary save the cleaned dataset to a pkl or csv file\n",
    "#df_cleandata.to_pickle('openIMIS csv/AllData_CleanData.pkl')\n",
    "#df_cleandata.to_csv('openIMIS csv/AllData_CleanData.csv')\n",
    "\n",
    "# if neccesary, show information about the columns of the cleaned dataset:\n",
    "# columns = list(df_cleandata)[:]\n",
    "# print(f'''The cleandata set is thus composed of {df_cleandata.shape[0]} rows and {df_cleandata.shape[1]} colums:''')\n",
    "# for i in columns:\n",
    "#     print(f''' - the {i} is {df_cleandata.dtypes[i]} and has {df_cleandata[i].notnull().sum()} non null values, \\\n",
    "#     {df_cleandata[i].isnull().sum()} null values, {df_cleandata[i].value_counts().count()} distinct values;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the full dataset, as it is no longer necessary, il allows to free memory\n",
    "del [[df_claim_si_concat, df_selection]]\n",
    "df_claim_si_concat=pd.DataFrame()\n",
    "df_selection = pd.DataFrame()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the labeled dataset\n",
    "label = df_cleandata['ReviewStatus']==RS_Reviewed\n",
    "\n",
    "conditions = [\n",
    "    (label),\n",
    "    ~(label)\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['Labeled', 'Not Labeled']\n",
    "\n",
    "# create a new column \n",
    "df_cleandata['Label']=np.select(conditions, values)\n",
    "\n",
    "# saved the labeled data in a new dataframe\n",
    "df_cleandata_labeled = df_cleandata.loc[df_cleandata['Label']=='Labeled'].copy()\n",
    "\n",
    "memStats_labeleddata = (df_cleandata_labeled.memory_usage()/1024/1024).sum()\n",
    "shape_labeleddata = df_cleandata_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of accepted/rejected \n",
    "accepted_by_MO = df_cleandata_labeled['RejectionReason']==RR_Accepted\n",
    "\n",
    "conditions = [\n",
    "    (accepted_by_MO),\n",
    "    ~(accepted_by_MO)\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['Accepted', 'Rejected']\n",
    "\n",
    "\n",
    "# create a new column related to 'Class' Accepted/Rejected:\n",
    "df_cleandata_labeled['Class']=np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to pickle\n",
    "df_cleandata_labeled.to_pickle('openIMIS csv/AllData_LabeledData.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation: illustrate the label/not labeled and Accepted/Rejected item counts\n",
    "figs = plt.figure(figsize=(12,4))\n",
    "ax1 = figs.add_subplot(121)\n",
    "items_count  = df_cleandata['Label'].value_counts(normalize=True).sort_index()\n",
    "sns.barplot(items_count.index, items_count.values, alpha=0.8)\n",
    "plt.xticks(range(2),['Labeled', 'Not Labeled'])\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "\n",
    "ax2 = figs.add_subplot(122)\n",
    "\n",
    "items_count  = df_cleandata_labeled['Class'].value_counts(normalize=True).sort_index()\n",
    "sns.barplot(items_count.index, items_count.values, alpha=0.8)\n",
    "plt.xticks(range(2),['Accepted', 'Rejected'])\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().autofmt_xdate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleandata_notlabeled = df_cleandata.loc[df_cleandata['Label']=='Not Labeled'].copy()\n",
    "\n",
    "memStats_notlabeled = (df_cleandata_notlabeled.memory_usage()/1024/1024).sum()\n",
    "shape_notlabeled = df_cleandata_notlabeled.shape\n",
    "\n",
    "# df_cleandata_notlabeled.to_pickle('openIMIS csv/AllData_NotLabeledData.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [[df_cleandata,df_cleandata_notlabeled]]\n",
    "df_cleandata=pd.DataFrame()\n",
    "df_cleandata_notlabeled=pd.DataFrame()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field selection\n",
    "\n",
    "As several columns were necessary only for visualisation reasons \n",
    "or for checking the status of a claim, these columns will be droped\n",
    "and we will continue working with only the selected columns\n",
    "Reasons: \n",
    "- the following columns have an UUID correspondence, and hence these IDs will be droped\n",
    "'ClaimID', 'ItemID', 'InsureeID', 'HFID', 'ClaimAdminId', 'FamilyID', 'LocationId','HFLocationId'\n",
    "(ClaimUUID', 'ItemUUID', 'InsureeUUID', 'HFUUID',  'ClaimAdminUUID', 'FamilyUUID','LocationUUID','HFLocationUUID')\n",
    "- some IDs are allready checked and no longer necessary \n",
    "'PolicyID','ProdID', 'FamHeadInsuree','HFId','InsureeHFID','ClaimItemID',\n",
    "- names, necessary only for visualisation, not usefull for AI model\n",
    "'ItemName','HFLocationName','ICDName','InsureeLocationName', \n",
    "- fields necessary only for checking the valuation of an item: 'ValidityFromReview', 'AuditUserIDReview'\n",
    "- communication fields: 'Explanation','Explanation', 'Justification','ClaimExplanation', 'Adjustment', 'ClaimCode',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colno = df_cleandata_labeled.shape[1]\n",
    "droped_cols = ['ClaimItemID','ClaimID','ItemID','InsureeID','HFID','ClaimAdminId','FamilyID',\\\n",
    "'LocationId','HFLocationId',\\\n",
    "'PolicyID','ProdID', 'FamHeadInsuree','HFId','InsureeHFID',\\\n",
    "'ItemName','HFLocationName','ICDName','InsureeLocationName',\\\n",
    "'ValidityFromReview', 'AuditUserIDReview']\n",
    "df_cleandata_labeled.drop(droped_cols, axis=1, inplace=True)               \n",
    "\n",
    "new_colno = df_cleandata_labeled.shape[1]\n",
    "\n",
    "df_cleandata_labeled.to_pickle('openIMIS csv/AllData_LabeledData_Selection.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: \n",
    "nb_total = shape_cleandata[0]\n",
    "nb_labeled = df_cleandata_labeled.shape[0]\n",
    "nb_accepted = accepted_by_MO.sum()\n",
    "nb_rejected = nb_labeled - nb_accepted\n",
    "print(f'''From {nb_total} records in the clean dataset:\n",
    "- {nb_labeled} are labeled (reviewed by a MO), representing {round(nb_labeled*100/nb_total,2)} % of all the data\n",
    "- {nb_total - nb_labeled} are not labeled (accepted without manual reviewing), representing {round((nb_total - nb_labeled)*100/nb_total,2)} % of all the data\n",
    "With respect to the labeled data, we can count:\n",
    "- {round(nb_accepted*100/nb_labeled,2)} % of labeled records were accepted ({nb_accepted})\n",
    "- {round(nb_rejected*100/nb_labeled,2)} % of labeled records were rejected ({nb_rejected}).\n",
    "\n",
    "Droped {colno-new_colno} columns (of {colno}, as no longer necessary). New dimensions of labeled dataset: \\\n",
    "{nb_labeled,new_colno}\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
